{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868929ac-481f-4684-a2dd-9116c0250939",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting dbldatagen\n  Downloading dbldatagen-0.4.0.post1-py3-none-any.whl (122 kB)\nInstalling collected packages: dbldatagen\nSuccessfully installed dbldatagen-0.4.0.post1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2885abf2-9760-402f-ad0c-11ac10f6fdf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting faker\n  Downloading Faker-26.0.0-py3-none-any.whl (1.8 MB)\nRequirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.9/site-packages (from faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\nInstalling collected packages: faker\nSuccessfully installed faker-26.0.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be504e2e-b0e5-4d23-9246-dbf07728d752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/capstone/txn_tbl/checkpoint_location/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af2e20b-8e15-434f-9a74-13ddeafbec89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/capstone/txn_tbl/latest/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f5e666-5a38-4182-8663-5207c074b827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "drop table if exists bronze.txn_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82872c9-7d40-4cf0-97ab-ebca3c3c9313",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: True"
     ]
    }
   ],
   "source": [
    "#create schema before running and also delete this location before dem\n",
    "\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/bronze.db/txn_tbl\" ,recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ddca400-cf57-46eb-a749-fcbf906ab91e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.datasets import make_regression\n",
    "import dbldatagen as dg\n",
    "from pyspark.sql import SparkSession\n",
    "from time import sleep\n",
    " \n",
    "class DataGenerator:\n",
    "    def __init__(self, num_customers=1000, num_branches=10, num_transactions=5000):\n",
    "        self.fake = faker.Faker()\n",
    "        self.num_customers = num_customers\n",
    "        self.num_branches = num_branches\n",
    "        self.num_transactions = num_transactions\n",
    "        self.spark = SparkSession.builder.appName(\"DataGeneration\").getOrCreate()\n",
    " \n",
    "    def format_phone_number(self):\n",
    "        phone_number = self.fake.phone_number()\n",
    "        digits = ''.join(filter(str.isdigit, phone_number))\n",
    "        return f\"({digits[:3]}) {digits[3:6]}-{digits[6:10]}\"\n",
    " \n",
    "    def random_date(self, start_date, end_date):\n",
    "        start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        return start_dt + timedelta(days=random.randint(0, (end_dt - start_dt).days))\n",
    " \n",
    "    def generate_customers(self):\n",
    "        names = [self.fake.name() for _ in range(self.num_customers)]\n",
    "        first_names = [name.split()[0] for name in names]\n",
    "        last_names = [name.split()[-1] for name in names]\n",
    "        addresses = [self.fake.address().replace('\\n', ', ') for _ in range(self.num_customers)]\n",
    "        email_providers = [\"gmail.com\", \"yahoo.com\", \"outlook.com\", \"hotmail.com\", \"aol.com\"]\n",
    "        emails = [f\"{first.lower()}.{last.lower()}@{random.choice(email_providers)}\" for first, last in zip(first_names, last_names)]\n",
    "        phone_numbers = [self.format_phone_number() for _ in range(self.num_customers)]\n",
    "        join_dates = sorted([self.random_date(\"2019-01-01\", \"2024-07-19\").strftime(\"%Y-%m-%d\") for _ in range(self.num_customers)])\n",
    "        last_updates = [self.random_date(\"2024-07-20\", \"2024-07-31\").strftime(\"%Y-%m-%d %H:%M:%S\") for _ in range(self.num_customers)]\n",
    "        credit_scores = [random.randint(500, 850) for _ in range(self.num_customers)]\n",
    "        customer_ids = [f\"C{index+1000:04d}\" for index in range(self.num_customers)]\n",
    " \n",
    "        customers_df = pd.DataFrame({\n",
    "            \"customer_id\": customer_ids,\n",
    "            \"name\": names,\n",
    "            \"email\": emails,\n",
    "            \"phone\": phone_numbers,\n",
    "            \"address\": addresses,\n",
    "            \"credit_score\": credit_scores,\n",
    "            \"join_date\": join_dates,\n",
    "            \"last_update\": last_updates\n",
    "        })\n",
    " \n",
    "        return self.spark.createDataFrame(customers_df)\n",
    " \n",
    "    def generate_branches(self):\n",
    "        branch_ids = [f\"B{index:04d}\" for index in range(self.num_branches)]\n",
    "        branch_names = random.choices([\"Downtown Branch\", \"Central Branch\", \"North Branch\", \"East Branch\", \"West Branch\"], k=self.num_branches)\n",
    "        cities = [self.fake.city() for _ in range(100)]\n",
    "        branch_locations = random.choices(cities, k=self.num_branches)\n",
    "        branch_timezones = random.choices([\"EST\", \"GMT\", \"PST\", \"AEST\"], k=self.num_branches)\n",
    " \n",
    "        branches_df = pd.DataFrame({\n",
    "            \"branch_id\": branch_ids,\n",
    "            \"name\": branch_names,\n",
    "            \"location\": branch_locations,\n",
    "            \"timezone\": branch_timezones\n",
    "        })\n",
    " \n",
    "        return self.spark.createDataFrame(branches_df)\n",
    " \n",
    "    def generate_transactions(self, customer_ids, branch_ids):\n",
    "        transaction_ids = [f\"T{index:04d}\" for index in range(5000, 10000)]\n",
    "        X, y = make_regression(n_samples=self.num_transactions, n_features=1, noise=10)\n",
    "        amounts = np.clip(np.abs(y), 1, 10000)\n",
    " \n",
    "        num_outliers = int(0.05 * len(amounts))\n",
    "        outliers_indices = np.random.choice(len(amounts), num_outliers, replace=False)\n",
    "        amounts[outliers_indices] = np.random.uniform(10000, 100000, size=num_outliers)\n",
    "        amounts = np.round(amounts, 2)\n",
    " \n",
    "        status_values = [\"completed\"] * (int(0.9 * self.num_transactions)) + [\"pending\"] * int(0.1 * self.num_transactions)\n",
    "        random.shuffle(status_values)\n",
    " \n",
    "        random_intervals = np.random.randint(0, 720, size=self.num_transactions)\n",
    "        cumulative_intervals = np.cumsum(random_intervals)\n",
    "        start_date = pd.Timestamp(\"2023-01-01\")\n",
    "        random_timestamps = [start_date + pd.Timedelta(minutes=int(m)) for m in cumulative_intervals]\n",
    " \n",
    " \n",
    "        transactions_df = pd.DataFrame({\n",
    "            \"transaction_id\": transaction_ids,\n",
    "            \"customer_id\": random.choices(customer_ids, k=self.num_transactions),\n",
    "            \"branch_id\": random.choices(branch_ids, k=self.num_transactions),\n",
    "            \"channel\": random.choices([\"ATM\", \"web\", \"mobile\", \"branch\"], k=self.num_transactions),\n",
    "            \"transaction_type\": random.choices([\"withdrawal\", \"deposit\", \"transfer\", \"payment\"], k=self.num_transactions),\n",
    "            \"amount\": amounts,\n",
    "            \"currency\": random.choices([\"USD\", \"EUR\", \"GBP\"], k=self.num_transactions),\n",
    "            \"timestamp\": random_timestamps,\n",
    "            \"status\": status_values\n",
    "        })\n",
    " \n",
    "        return self.spark.createDataFrame(transactions_df)\n",
    " \n",
    "# Usage\n",
    "data_gen = DataGenerator()\n",
    " \n",
    "customers_spark_df = data_gen.generate_customers()\n",
    "branches_spark_df = data_gen.generate_branches()\n",
    "transactions_spark_df = data_gen.generate_transactions(customers_spark_df.select(\"customer_id\").rdd.flatMap(lambda x: x).collect(),\n",
    "                                                       branches_spark_df.select(\"branch_id\").rdd.flatMap(lambda x: x).collect())\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1ad643-845e-470a-bdf3-9a9e560d3e45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_spark_df.write.format(\"delta\").mode('overwrite').save(\"dbfs:/FileStore/capstone/cust_tbl/\")\n",
    "branches_spark_df.write.format(\"delta\").mode('overwrite').save(\"dbfs:/FileStore/capstone/branches_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ba2cdb-c372-4daf-a310-894942529d0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Generating dummy data with anamolies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61e656c-9103-470b-bd7e-66fc3120d40d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1000 records. Waiting 10 seconds...\nWrote 1000 records. Waiting 10 seconds...\nWrote 1000 records. Waiting 10 seconds...\nWrote 1000 records. Waiting 10 seconds...\nWrote 1000 records. Waiting 10 seconds...\nCompleted writing txn data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "transactions_pandas_df = transactions_spark_df.orderBy(\"transaction_id\").toPandas()\n",
    "\n",
    "# Split into 5 chunks \n",
    "chunks = np.array_split(transactions_pandas_df, 5)\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Convert Pandas DataFrame back to Spark DataFrame\n",
    "    chunk_spark_df = spark.createDataFrame(chunk)\n",
    "    \n",
    "    # Write the chunk to Delta table\n",
    "    chunk_spark_df.write.format(\"delta\").mode(\"append\").save(\"dbfs:/FileStore/capstone/txn_tbl/latest/\")\n",
    "    print(f\"Wrote {len(chunk)} records. Waiting 10 seconds...\")\n",
    "    \n",
    "    # Sleep for 10 seconds\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"Completed writing txn data\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1679068450359519,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "dataset final",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
